{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.contrib.layers import fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_seq_length=30\n",
    "trainDF = pd.read_csv('C:/Users/Administrator.NBJXUEJUN-LI/Desktop/project/RProj/my code/jdProj/save/traindf.csv',\n",
    "                      header=0,encoding='gbk' )\n",
    "testDF = pd.read_csv('C:/Users/Administrator.NBJXUEJUN-LI/Desktop/project/RProj/my code/jdProj/save/testdf.csv',\n",
    "                      header=0,encoding='gbk' )\n",
    "predDF = pd.read_csv('C:/Users/Administrator.NBJXUEJUN-LI/Desktop/project/RProj/my code/jdProj/save/preddf.csv',\n",
    "                      header=0,encoding='gbk' )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prodmap= pd.read_csv('C:/Users/Administrator.NBJXUEJUN-LI/Desktop/project/RProj/my code/jdProj/save/prodEmbIndxMP.csv',\n",
    "                      header=0,encoding='gbk' )\n",
    "usermap= pd.read_csv('C:/Users/Administrator.NBJXUEJUN-LI/Desktop/project/RProj/my code/jdProj/save/userEmbIndxMP.csv',\n",
    "                      header=0,encoding='gbk' )\n",
    "brandmap= pd.read_csv('C:/Users/Administrator.NBJXUEJUN-LI/Desktop/project/RProj/my code/jdProj/save/brandEmbIndxMP.csv',\n",
    "                      header=0,encoding='gbk' )\n",
    "targmap= pd.read_csv('C:/Users/Administrator.NBJXUEJUN-LI/Desktop/project/RProj/my code/jdProj/save/targProdEmbIndxMP.csv',\n",
    "                      header=0,encoding='gbk' )\n",
    "\n",
    "prodNum = prodmap.index[-1]+1\n",
    "userNum = usermap.index[-1]+1\n",
    "brandNum = brandmap.index[-1]+1\n",
    "targNum = targmap.index[-1]+1\n",
    "\n",
    "prodmap=dict(zip(prodmap.mp ,prodmap.index))\n",
    "usermap=dict(zip(usermap.mp ,usermap.index))\n",
    "brandmap=dict(zip(brandmap.mp ,brandmap.index))\n",
    "targmap=dict(zip(targmap.mp,targmap.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saticDefault = ['0']*23\n",
    "def indexSplit(index,max_seq_length,mapdict):\n",
    "    tmp = index.split(\"<-\")[::-1]\n",
    "    ret = [mapdict.get(int(tmp[i]),0) if i<len(tmp)else 0 for i in range(max_seq_length)]\n",
    "    return ret\n",
    "def staticFeatSplit(staticFeat,max_seq_length):\n",
    "    tmp = staticFeat.split(\"<-\")[::-1]\n",
    "    ret = [tmp[i].split(\"#\") if i<len(tmp)else saticDefault for i in range(max_seq_length)]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainProdSeq = np.array([indexSplit(prod,max_seq_length,prodmap) for prod in trainDF.skuidx],dtype=np.int32)\n",
    "trainUserSeq = np.array([[usermap.get(user,0)]*max_seq_length for user in trainDF.user_id],dtype=np.int32)\n",
    "trainBrandSeq = np.array([indexSplit(brand,max_seq_length,brandmap) for brand in trainDF.brandidx],dtype=np.int32)\n",
    "trainStaticFeatSeq = np.array([staticFeatSplit(staticFeat,max_seq_length) for staticFeat in trainDF.static_feat],dtype=np.float32)\n",
    "ySeq = np.array([targmap.get(int(label),0) for label in trainDF.label],dtype=np.int32)\n",
    "trainSeqLength = np.array(trainDF.seq_length,dtype=np.int32)\n",
    "\n",
    "testProdSeq = np.array([indexSplit(prod,max_seq_length,prodmap) for prod in testDF.skuidx],dtype=np.int32)\n",
    "testUserSeq = np.array([[usermap.get(user,0)]*max_seq_length for user in testDF.user_id],dtype=np.int32)\n",
    "testBrandSeq = np.array([indexSplit(brand,max_seq_length,brandmap) for brand in testDF.brandidx],dtype=np.int32)\n",
    "testStaticFeatSeq = np.array([staticFeatSplit(staticFeat,max_seq_length) for staticFeat in testDF.static_feat],dtype=np.float32)\n",
    "ySeqtest = np.array([targmap.get(int(label),0) for label in testDF.label],dtype=np.int32)\n",
    "testSeqLength = np.array(testDF.seq_length,dtype=np.int32)\n",
    "\n",
    "predProdSeq = np.array([indexSplit(prod,max_seq_length,prodmap) for prod in predDF.skuidx],dtype=np.int32)\n",
    "predUserSeq = np.array([[usermap.get(user,0)]*max_seq_length for user in predDF.user_id],dtype=np.int32)\n",
    "predBrandSeq = np.array([indexSplit(brand,max_seq_length,brandmap) for brand in predDF.brandidx],dtype=np.int32)\n",
    "predStaticFeatSeq = np.array([staticFeatSplit(staticFeat,max_seq_length) for staticFeat in predDF.static_feat],dtype=np.float32)\n",
    "predSeqLength = np.array(predDF.seq_length,dtype=np.int32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#construct the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Prod_size  = prodNum\n",
    "user_size  = userNum\n",
    "brand_size = brandNum\n",
    "\n",
    "prod_embedding_size  =20\n",
    "user_embedding_size  =20\n",
    "brand_embedding_size =20\n",
    "\n",
    "max_step_num    = max_seq_length\n",
    "static_feat_dim = 23\n",
    "n_neurons       = 50\n",
    "\n",
    "n_output        = targNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prod_embeddings  = tf.Variable(tf.random_uniform([Prod_size,prod_embedding_size], -1.0, 1.0))\n",
    "user_embeddings  = tf.Variable(tf.random_uniform([user_size,user_embedding_size], -1.0, 1.0))\n",
    "brand_embeddings = tf.Variable(tf.random_uniform([brand_size,brand_embedding_size], -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prod_idx  = tf.placeholder(tf.int32, shape=[None, max_step_num])\n",
    "user_idx  = tf.placeholder(tf.int32, shape=[None, max_step_num])\n",
    "brand_idx = tf.placeholder(tf.int32, shape=[None, max_step_num])\n",
    "\n",
    "seq_length = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "static_feat = tf.placeholder(tf.float32, shape=[None, max_step_num,static_feat_dim])\n",
    "prod_embed  = tf.nn.embedding_lookup(prod_embeddings , prod_idx)\n",
    "user_embed  = tf.nn.embedding_lookup(user_embeddings , user_idx)\n",
    "brand_embed = tf.nn.embedding_lookup(brand_embeddings, brand_idx)\n",
    "\n",
    "y = tf.placeholder(tf.int32,[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_feat=tf.concat([static_feat,prod_embed,user_embed,brand_embed], 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    " \n",
    "basic_cell = tf.contrib.rnn.DropoutWrapper(basic_cell,input_keep_prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, final_feat, dtype=tf.float32,sequence_length=seq_length)\n",
    "logits = fully_connected(states, n_output, activation_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=alpha)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct = tf.nn.in_top_k(logits,y,1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feed data and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs=1000\n",
    "batch_size=10\n",
    "batch_num = len(ySeq)//batch_size\n",
    "sessMode  = \"init_train\"\n",
    "load_path = \"old\"\n",
    "save_path = \"old\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0126263 0.0\n",
      "0.128788 0.0\n",
      "0.128788 0.010101\n",
      "0.121212 0.010101\n",
      "0.123737 0.020202\n",
      "0.123737 0.020202\n",
      "0.123737 0.020202\n",
      "0.126263 0.020202\n",
      "0.126263 0.030303\n",
      "0.123737 0.030303\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Parent directory of /Users/snakepointid/Documents/project/RProj/my code/r相关函数/save/old/test.ckpt doesn't exist, can't save.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e50278b4d6b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                         seq_length: testSeqLength,y:ySeqtest})\n\u001b[1;32m     22\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0msave_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"/Users/snakepointid/Documents/project/RProj/my code/r相关函数/save/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/test.ckpt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0msessMode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"continue_train\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"/Users/snakepointid/Documents/project/RProj/my code/r相关函数/save/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mload_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/test.ckpt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIsDirectory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m       raise ValueError(\n\u001b[0;32m-> 1354\u001b[0;31m           \"Parent directory of {} doesn't exist, can't save.\".format(save_path))\n\u001b[0m\u001b[1;32m   1355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[0msave_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Parent directory of /Users/snakepointid/Documents/project/RProj/my code/r相关函数/save/old/test.ckpt doesn't exist, can't save."
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    if sessMode==\"init_train\":\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            for batch in range(batch_num):\n",
    "                prod_batch       = trainProdSeq[batch*batch_size:(batch+1)*batch_size]\n",
    "                user_batch       = trainUserSeq[batch:batch+batch_size]\n",
    "                brand_batch      = trainBrandSeq[batch:batch+batch_size]\n",
    "                static_batch     = trainStaticFeatSeq[batch:batch+batch_size]\n",
    "                seq_length_batch = trainSeqLength[batch:batch+batch_size]\n",
    "                y_batch          = ySeq[batch:batch+batch_size]\n",
    "                sess.run(training_op, feed_dict={\n",
    "                    prod_idx: prod_batch,user_idx:user_batch,brand_idx:brand_batch,static_feat:static_batch, \n",
    "                    seq_length: seq_length_batch,y:y_batch})\n",
    "            if epoch%100==0:\n",
    "                train_acc = accuracy.eval(feed_dict={\n",
    "                        prod_idx: trainProdSeq,user_idx:trainUserSeq,brand_idx:trainBrandSeq,static_feat:trainStaticFeatSeq, \n",
    "                        seq_length: trainSeqLength,y:ySeq})\n",
    "                test_acc  = accuracy.eval(feed_dict={\n",
    "                        prod_idx: testProdSeq,user_idx:testUserSeq,brand_idx:testBrandSeq,static_feat:testStaticFeatSeq, \n",
    "                        seq_length: testSeqLength,y:ySeqtest})\n",
    "                print(train_acc,test_acc)\n",
    "        save_path = saver.save(sess,\"/Users/snakepointid/Documents/project/RProj/my code/r相关函数/save/\"+save_path+\"/test.ckpt\")\n",
    "    elif sessMode==\"continue_train\":\n",
    "        saver.restore(sess,\"/Users/snakepointid/Documents/project/RProj/my code/r相关函数/save/\"+load_path+\"/test.ckpt\")\n",
    "        for epoch in range(n_epochs):\n",
    "            nt,accuracy_eval = sess.run([training_op,accuracy], feed_dict={\n",
    "                    prod_idx: prod_batch,user_idx:user_batch,brand_idx:brand_batch,static_feat:static_batch, \n",
    "                    seq_length: seq_length_batch,y:y_batch})\n",
    "            print(accuracy_eval)\n",
    "        save_path = saver.save(sess,\"/Users/snakepointid/Documents/project/RProj/my code/r相关函数/save/\"+save_path+\"/test.ckpt\")\n",
    "    elif sessMode==\"predict\":\n",
    "        saver.restore(sess,\"/Users/snakepointid/Documents/project/RProj/my code/r相关函数/save/\"+load_path+\"/test.ckpt\")\n",
    "        logits_eval = logits.eval(feed_dict={\n",
    "                    prod_idx: prod_batch,user_idx:user_batch,brand_idx:brand_batch,static_feat:static_batch, \n",
    "                    seq_length: seq_length_batch,y:y_batch})\n",
    "        print(logits_eval.argmax(1))\n",
    "    else:\n",
    "        print(\"wrong session type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
